sample: 
  logging_dir: logs
  enable_xformers_memory_efficient_attention: false
  seed: 42 
  resolution: 512
  pretrained_model_name_or_path: black-forest-labs/FLUX.1-Fill-dev
  output_dir: ../global_seed_42/batch_2/local_seed_0 
  sample_batch_size: 8 
  mixed_precision: bf16
  dataloader_num_workers: 8 
  save_step: 320 
  generator_seed: 0 
  skip_batches: null 
  max_steps: 250 
  train_json: /path/to/sample.json 

train:
  revision: null
  variant: null
  num_train_epochs: 10000
  resume_from_checkpoint: null
  max_train_steps: null
  gradient_accumulation_steps: 1
  scale_lr: false
  lr_scheduler: constant
  lr_warmup_steps: 500
  lr_num_cycles: 1
  lr_power: 1.0
  optimizer: AdamW
  use_8bit_adam: false
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_weight_decay: 0.01
  adam_epsilon: 1.0e-08
  max_grad_norm: 1.0
  push_to_hub: false
  hub_token: null
  hub_model_id: null
  logging_dir: logs
  allow_tf32: false
  enable_xformers_memory_efficient_attention: false
  set_grads_to_none: false
  dataset_name: null
  dataset_config_name: null
  image_column: image
  conditioning_image_column: conditioning_image
  caption_column: text
  max_train_samples: null
  proportion_empty_prompts: 0
  validation_prompt:
  - A cake on the table.
  validation_image:
  - ./test_image.jpg
  validation_mask:
  - ./test_mask.jpg
  num_validation_images: 1
  wandb_id: null
  random_mask: false
  weighting_scheme: none
  logit_mean: 0.0
  logit_std: 1.0
  mode_scale: 1.29
  height: 512
  width: 512
  max_sequence_length: 512
  seed: 42
  gradient_checkpointing: true 
  checkpoints_total_limit: 1 # this will only delete the "pytorch_models" in older checkpoints but keep the "transformer"
  pretrained_model_name_or_path: black-forest-labs/FLUX.1-Fill-dev
  # train_data_dir is no longer important as we save the data in BrushData to pngs
  train_data_dir: ~/.cache/huggingface/hub/datasets--random123123--BrushData/snapshots/333cdc6dc928b6efdbe25c0b3943d1c67a85fc8f
  train_json_dir: /path/to/results.json 
  resolution: 512
  learning_rate: 1.0e-05 
  train_batch_size: 1 
  tracker_project_name: flux
  validation_steps: 200 
  checkpointing_steps: 1000 
  train_steps: 2000
  dataloader_num_workers: 8
  beta_dpo: 2000 
  dpo_loss_weight: 1.0
  mse_loss_weight: 0.0
  output_dir: /root/test_env/runs/debug/flux 
  mixed_precision: bf16
  report_to: wandb
  guidance_scale: 1
  train_base_model: true
  dpo_new: false # Specify the actual losing sample as winning sample, and the actual winning sample as losing sample
  dpo_mask: true
  do_test: false
  note: null
  metrics:
    enable: true 
    score_file: /path/to/results_with_ensemble.json 
    metric: ensemble 
    scaling: 16 
  lora:
    enable: false
    lora_layers: null
    rank: 64
    alpha: 128

